---
title: 'ENSC650 lecture 03: Linear Regression'
author: Akihiko Mori
date: '2021-09-30'
slug: []
categories: []
tags:
  - R
---
# Correlation $\rho_{xy}$
statistical relationship between two variables\

$-1 \le r_{xy} \le +1$.

$$
\begin{align}\rho_{X,Y} &= \operatorname{cor}(X,Y) \\
&= {\operatorname{cov}(X,Y) \over \sigma_X \sigma_Y} \\
&= {\operatorname{E}[(X-\mu_X)(Y-\mu_Y)] \over \sigma_X\sigma_Y}\\
&= \frac {\operatorname{E}(XY)-\operatorname{E}(X)\operatorname{E}(Y)}{ \sqrt{\operatorname{E}(X^2)-\operatorname{E}(X)^2}\cdot \sqrt{\operatorname{E}(Y^2)-\operatorname{E}(Y)^2}}
\end{align}$$

$$\begin{align}
X,Y \text{ independent} \quad & \Rightarrow \quad \rho_{X,Y} = 0 \quad (X,Y \text{ uncorrelated})\\
\rho_{X,Y} = 0 \quad (X,Y \text{ uncorrelated})\quad & \nRightarrow \quad X,Y \text{ independent}
\end{align}$$
```{r}
#by hand 
x <- c(25, 27, 29)
y <- c(5, 15, 9)
xdev <- x - mean(x)
ydev <- y - mean(y)
xdev_ydev <- xdev * ydev
sum_xdev_ydev <- sum(xdev_ydev)
cov_xy <- (1 / (3 - 1)) * sum_xdev_ydev
stnd.dev <- sd(x)*sd(y)
cov_xy/stnd.dev

# built-in
cor(x,y)
```

# Test of Correlation
a correlation test is used to test whether the correlation (denoted $\rho$) between 2 variables is significantly different from 0 or not in the population.

* $H_0: \rho = 0 $(meaning that there is no linear relationship between the two variables)\
* $H_a: \rho \neq 0$(meaning that there is a linear relationship between the two variables)

$$t = \rho\sqrt{\frac{N-2}{1-\rho^2}} \sim \text{t-distribution(N-2)}$$
d.f.= $N-2$

```{r}
r <- cor(x,y)
N <- length(x)
t = r*sqrt((N-2)/(1-r^2))
t
pt(t,df=N-2, lower.tail = FALSE)*2
cor.test(x,y)
```


# Autocorrelation
In time series data, assess whether a time series is dependent on its past.
$$\rho_{k} = \frac{\sum_{i=1}^{N-k}(X_{i} - \bar{X})(X_{i+k} - 
         \bar{X})} {\sum_{i=1}^{N}(X_{i} - \bar{X})^{2} }
$$
```{r}
#define data
x <- c(22, 24, 25, 25, 28, 29, 34, 37, 40, 44, 51, 48, 47, 50, 51)
library(tseries)

#calculate autocorrelations
acf(x,pl=F)
#plot
acf(x)
```


# Correlation Matrix
The correlation matrix of $n$ random variables $X_{1},\ldots ,X_{n}$ is the $n\times n$ matrix whose $(i,j)$ entry is ${\displaystyle \operatorname {corr} (X_{i},X_{j})}$.
Thus the diagonal entries are all identically unity or one, and the matrix is symmetric. 
```{r}
source("http://www.sthda.com/upload/rquery_cormat.r")
library("corrplot")
mydata <- mtcars[, c(1,3,4,5,6,7)]
cor(mydata)
rquery.cormat(mydata)
```

# Regression
a linear relation between a dependent variable $y$ and one or more independent variables, $x$.\

## Simple Linear Model
$$Y_i = \alpha + \beta X_i + \varepsilon_i.$$

* $Y$ is the random response for the i-th case\
* $\alpha$, $\beta$ are parameters\
* $X_i$ is a know constant, the value of the predictor varialbe for the i-th case\
* $\epsilon_i$ is a random error term, such that:\
  + $\operatorname{E}(\epsilon_i)=0$\
  + $\operatorname{\sigma^2}(\epsilon_i)=\sigma^2$\
  + $\operatorname{\sigma}(Y_i,Y_j), \forall i,j\ni i\neq j$.
  
$$\begin{align}
\widehat\beta &= \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i -\bar{y})}{ \sum_{i=1}^n (x_i - \bar{x})^2 } \\
            &= \frac{ s_{x, y} }{ s^2_{x} } \\
            &= r_{xy} \frac{s_y}{s_x}. \\
 \widehat\alpha & = \bar{y} - (  \widehat\beta\,\bar{x}), \\
\end{align}$$

* $\bar x$ and $\bar y$ as the average of the $x_i$ and $y_i$, respectively\
* $r_xy$ as the sample correlation coefficient between $x$ and $y$\
* $s_x$ and $s_y$ as the uncorrected sample standard deviations of $x$ and $y$\
* $s^2_x$ and $s_{x,y}$ as the sample variance and sample covariance, respectively

```{r}
x <- mtcars$wt
y <- mtcars$mpg
lm(x~y, data = data.frame(x,y))
```

### Homework: $\sigma_e^2 = \sigma_y^2(1-\rho_{xy})$

$$\begin{align}
\sigma_e^2 &= Var(e)\\
&= Var(Y-\hat{Y})\\
&= Var\left(\left(Y-\bar{Y}\right)-\left(\hat{Y}-\bar{Y}\right)\right)\\
&= Var(Y-\bar{Y})+Var(\hat{Y}-\bar{Y})-2Cov(Y-\bar{Y},\hat{Y}-\bar{Y})\\
&= Var(Y-\bar{Y})+Var(\alpha+\beta X-\bar{Y})-2Cov(Y-\bar{Y},\alpha+\beta X-\bar{Y})\\
&= Var(Y-\bar{Y})+Var(\bar{Y}-\beta \bar{X}+\beta X-\bar{Y})-2Cov(Y-\bar{Y},\bar{Y}-\beta \bar{X}+\beta X-\bar{Y})\\
&= Var(Y-\bar{Y}) + Var(\beta \bar{X}-\beta X)-2Cov(Y-\bar{Y},\beta \bar{X}-\beta X)\\
&= Var(Y-\bar{Y}) + \beta^2Var(X-\bar{X})-2\beta Cov(Y-\bar{Y},X-\bar{X})\\
&= Var(Y) + \beta^2Var(X)-2\beta Cov(Y,X)\\
&= Var(Y) + \left(\frac{Cov(X,Y)}{Var(X)}\right)^2Var(X)-2\left(\frac{Cov(X,Y)}{Var(X)}\right) Cov(Y,X)\\
&= Var(Y) + \frac{Cov(X,Y)^2}{Var(X)}-\frac{2Cov(X,Y)^2}{Var(X)}\\
&= Var(Y) -\frac{Cov(X,Y)^2}{Var(X)}\\
&= Var(Y)\left[1 -\left(\frac{Cov(X,Y)^2}{Var(X)Var(Y)}\right)\right]\\
&= Var(Y)\left[1 -\left(\frac{Cov(X,Y)}{\sqrt{Var(X)}\sqrt{Var(Y)}}\right)^2\right]\\
&= \sigma_y^2 \left(1 - \rho_{xy}^2\right)
\end{align}$$

# Decomposion of Variance
law of total variance
$$\operatorname{Var}[X]=\operatorname{E}(\operatorname{Var}[X\mid Y])+\operatorname{Var}(\operatorname{E}[X\mid Y]).$$
the "unexplained" and the "explained" components of the variance.\
The conditional expectation $\operatorname E(X\mid Y)$, and the conditional variance $\operatorname{Var}(X\mid Y)$

 
Proof:
The definition of Variance is:
$$\operatorname{Var}[Y] = \operatorname{E}[Y^2] - \operatorname{E}[Y]^2$$
Then applying the law of total expectation,
$$\begin{align}
\operatorname{E}[Y^2] &= \operatorname{E} \left[\operatorname{Var}[Y\mid X] + [\operatorname{E}[Y\mid X]]^2\right]\\
\operatorname{E}[Y^2] - \operatorname{E}[Y]^2 &= \operatorname{E} \left[\operatorname{Var}[Y\mid X] + [\operatorname{E}[Y\mid X]]^2\right] - [\operatorname{E} [\operatorname{E}[Y\mid X]]]^2\\
&= \left(\operatorname{E} [\operatorname{Var}[Y\mid X]]\right) + \left(\operatorname{E} [\operatorname{E}[Y\mid X]^2] - \operatorname{E} [\operatorname{E}[Y\mid X]]^2\right)\\
&= \operatorname{E} [\operatorname{Var}[Y\mid X]] + \operatorname{Var} [\operatorname{E}[Y\mid X]]
\end{align}$$

Similar decomposition:
$$\mathit{SS}_\text{total} = \mathit{SS}_\text{between} + \mathit{SS}_\text{within},$$
$$\mathit{SS}_\text{total} = \mathit{SS}_\text{regression} + \mathit{SS}_\text{residual}.$$

$$\begin{align}
SST &= \sum\left(Y-\bar{Y}\right)^2\\
&= \sum\left(Y - \hat{Y} + \hat{Y} - \bar{Y}\right)^2\\
&= \sum\left[\left(Y - \hat{Y}\right)^2 + \left(\hat{Y} - \bar{Y}\right)^2 + 2\left(Y - \hat{Y}\right)\left(\hat{Y} - \bar{Y}\right)\right]\\
&= \sum\left(Y - \hat{Y}\right)^2 + \sum\left(\hat{Y} - \bar{Y}\right)^2 + 2\sum\left(Y - \hat{Y}\right)\left(\hat{Y} - \bar{Y}\right)\\
&= SSE + SSR + 0
\end{align}$$

$$R^2 = \frac{SSR}{SST} = 1-\frac{SSE}{SST}$$

# Significance test of Regression
$H_0: \hat\beta = 0$\
$H_a: \hat\beta \neq 0$, at least one of $\beta$

F: Test statistic
$$\begin{align}
F &= \frac{\frac{SSR}{k}}{\frac{SSE}{N-k-1}}\\
&= \frac{\frac{R^2}{k}}{\frac{1-R^2}{N-k-1}}
\end{align}$$
```{r}
mfit <- lm(mpg~am + cyl + wt + hp, data = mtcars)
summary(mfit)
```

