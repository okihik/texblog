---
title: 'ENSC650 lecture 03: Linear Regression'
author: Akihiko Mori
date: '2021-09-30'
slug: []
categories: []
tags:
  - R
---
# Correlation $\rho_{xy}$
statistical relationship between two variables
$-1 \le r_{xy} \le +1$.
$$
\begin{align}\rho_{X,Y} &= \operatorname{cor}(X,Y) \\
&= {\operatorname{cov}(X,Y) \over \sigma_X \sigma_Y} \\
&= {\operatorname{E}[(X-\mu_X)(Y-\mu_Y)] \over \sigma_X\sigma_Y}\\
&= \frac {\operatorname{E}(XY)-\operatorname{E}(X)\operatorname{E}(Y)}{ \sqrt{\operatorname{E}(X^2)-\operatorname{E}(X)^2}\cdot \sqrt{\operatorname{E}(Y^2)-\operatorname{E}(Y)^2}}
\end{align}$$
$$\begin{align}

X,Y \text{ independent} \quad & \Rightarrow \quad \rho_{X,Y} = 0 \quad (X,Y \text{ uncorrelated})\\
\rho_{X,Y} = 0 \quad (X,Y \text{ uncorrelated})\quad & \nRightarrow \quad X,Y \text{ independent}
\end{align}$$
```{r}
#by hand 
x <- c(25, 27, 29)
y <- c(5, 15, 9)
xdev <- x - mean(x)
ydev <- y - mean(y)
xdev_ydev <- xdev * ydev
sum_xdev_ydev <- sum(xdev_ydev)
cov_xy <- (1 / (3 - 1)) * sum_xdev_ydev
stnd.dev <- sd(x)*sd(y)
cov_xy/stnd.dev

# built-in
cor(x,y)
```

# Test of Correlation
a correlation test is used to test whether the correlation (denoted $\rho$) between 2 variables is significantly different from 0 or not in the population.

* $H_0: \rho = 0 $(meaning that there is no linear relationship between the two variables)\
* $H_a: \rho \neq 0$(meaning that there is a linear relationship between the two variables)

$$t = \rho\sqrt{\frac{N-2}{1-\rho^2}} \sim \text{t-distribution(N-2)}$$
d.f.= $N-2$

```{r}
r <- cor(x,y)
N <- length(x)
t = r*sqrt((N-2)/(1-r^2))
t
pt(t,df=N-2, lower.tail = FALSE)*2
cor.test(x,y)
```


# Autocorrelation
In time series data, assess whether a time series is dependent on its past.
$$\rho_{k} = \frac{\sum_{i=1}^{N-k}(X_{i} - \bar{X})(X_{i+k} - 
         \bar{X})} {\sum_{i=1}^{N}(X_{i} - \bar{X})^{2} }
$$
```{r}
#define data
x <- c(22, 24, 25, 25, 28, 29, 34, 37, 40, 44, 51, 48, 47, 50, 51)
library(tseries)

#calculate autocorrelations
acf(x,pl=F)
#plot
acf(x)
```


# Correlation Matrix
The correlation matrix of $n$ random variables $X_{1},\ldots ,X_{n}$ is the $n\times n$ matrix whose $(i,j)$ entry is ${\displaystyle \operatorname {corr} (X_{i},X_{j})}$.
Thus the diagonal entries are all identically unity or one, and the matrix is symmetric. 
```{r}
source("http://www.sthda.com/upload/rquery_cormat.r")
library("corrplot")
mydata <- mtcars[, c(1,3,4,5,6,7)]
cor(mydata)
rquery.cormat(mydata)
```

#Regression


```{r}

```


# Decomposion of Variance
law of total variance
$$\operatorname{Var}[X]=\operatorname{E}(\operatorname{Var}[X\mid Y])+\operatorname{Var}(\operatorname{E}[X\mid Y]).$$
the "unexplained" and the "explained" components of the variance.\
The conditional expectation $\operatorname E(X\mid Y)$, and the conditional variance $\operatorname{Var}(X\mid Y)$

 
Proof:
The definition of Variance is:
$$\operatorname{Var}[Y] = \operatorname{E}[Y^2] - \operatorname{E}[Y]^2$$
Then applying the law of total expectation,
$$\begin{align}
\operatorname{E}[Y^2] &= \operatorname{E} \left[\operatorname{Var}[Y\mid X] + [\operatorname{E}[Y\mid X]]^2\right]\\
\operatorname{E}[Y^2] - \operatorname{E}[Y]^2 &= \operatorname{E} \left[\operatorname{Var}[Y\mid X] + [\operatorname{E}[Y\mid X]]^2\right] - [\operatorname{E} [\operatorname{E}[Y\mid X]]]^2\\
&= \left(\operatorname{E} [\operatorname{Var}[Y\mid X]]\right) + \left(\operatorname{E} [\operatorname{E}[Y\mid X]^2] - \operatorname{E} [\operatorname{E}[Y\mid X]]^2\right)\\
&= \operatorname{E} [\operatorname{Var}[Y\mid X]] + \operatorname{Var} [\operatorname{E}[Y\mid X]]
\end{align}$$

Similar decomposition:
$$\mathit{SS}_\text{total} = \mathit{SS}_\text{between} + \mathit{SS}_\text{within},$$
$$\mathit{SS}_\text{total} = \mathit{SS}_\text{regression} + \mathit{SS}_\text{residual}.$$

$$\begin{align}
SST &= \sum\left(Y-\bar{Y}\right)^2\\
&= \sum\left(Y - \hat{Y} + \hat{Y} - \bar{Y}\right)^2\\
&= \sum\left[\left(Y - \hat{Y}\right)^2 + \left(\hat{Y} - \bar{Y}\right)^2 + 2\left(Y - \hat{Y}\right)\left(\hat{Y} - \bar{Y}\right)\right]\\
&= \sum\left(Y - \hat{Y}\right)^2 + \sum\left(\hat{Y} - \bar{Y}\right)^2 + 2\sum\left(Y - \hat{Y}\right)\left(\hat{Y} - \bar{Y}\right)\\
&= SSE + SSR + 0
\end{align}$$

$$R^2 = \frac{SSR}{SST} = 1-\frac{SSE}{SST}$$

# Significance test of Regression
$H_0: \hat\beta = 0$\
$H_a: \hat\beta \neq 0$, at least one of $\beta$

F: Test statistic
$$\begin{align}
F &= \frac{\frac{SSR}{k}}{\frac{SSE}{N-k-1}}\\
&= \frac{\frac{R^2}{k}}{\frac{1-R^2}{N-k-1}}
\end{align}$$
```{r}
mfit <- lm(mpg~am + cyl + wt + hp, data = mtcars)
summary(mfit)
```

